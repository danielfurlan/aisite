{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Logistic Regression \n\n\n","metadata":{"id":"gC5z6Kr8Pa1E"}},{"cell_type":"markdown","source":"\n\n<img src=\"https://raw.githubusercontent.com/danielfurlan/aisite/master/LogisticRegression/dist2d.png\"\nalign = \"center\"\n style=\"float:right;width:5px;height:5px;\">\n\n\n","metadata":{"id":"TzNcKPWISDPD"}},{"cell_type":"markdown","source":"# Overview\n\nIn this notebook you will check:\n\n\n1.   Data Visualization\n2.   Build a Logistic Regression from scratch\n3.   Use a built-in Logistic Regression model from Scikit-Learn\n4.   Use BootStrap Sampling with Confidence Interval\n\n","metadata":{"id":"V0nnayEyP1sq"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom math import exp\nimport seaborn as sns\nimport plotly.graph_objects as go\nimport time\nimport random\n\nrandom.seed(3)\n\n!wget \"https://drive.google.com/uc?id=15WAD9_4CpUK6EWmgWVXU8YMnyYLKQvW8&export=download\" -O data.csv -q\n# !wget \"https://www.kaggle.com/danielfmfurlan/graduate-admission/data=download\" -O data.csv -q\n# Load the data\ndata = pd.read_csv(\"data.csv\")\ndata.head()","metadata":{"id":"OqDIVXEpWFdN","outputId":"0d68efb8-4a3b-423c-8db3-c6425dd27e2b","execution":{"iopub.status.busy":"2022-03-04T14:22:16.923907Z","iopub.execute_input":"2022-03-04T14:22:16.925924Z","iopub.status.idle":"2022-03-04T14:22:19.944158Z","shell.execute_reply.started":"2022-03-04T14:22:16.925873Z","shell.execute_reply":"2022-03-04T14:22:19.943070Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"data.describe()","metadata":{"id":"djXVP5PG28Hd","outputId":"04863624-dd4c-4948-8363-ef6fed48c8b9","execution":{"iopub.status.busy":"2022-03-04T14:22:19.946066Z","iopub.execute_input":"2022-03-04T14:22:19.946756Z","iopub.status.idle":"2022-03-04T14:22:19.982505Z","shell.execute_reply.started":"2022-03-04T14:22:19.946701Z","shell.execute_reply":"2022-03-04T14:22:19.981775Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Lets take a look at some graphs to better visualize these statistics","metadata":{"id":"3xvNrd1j3Imo"}},{"cell_type":"code","source":"plt.figure(figsize=(15,10))\nplt.title(\"Distribution plot for 2 dimensions\")\nsns.kdeplot(\n    data=data, x=\"Age\", y=\"EstimatedSalary\", hue=\"Purchased\",\n    levels=75, thresh=0.2, fill = False, cmap = \"mako\"\n)\nsns.set_style(\"dark\")\nplt.axhline(y = 106000, alpha = 0.3,linestyle = \"--\")\nplt.axvline(x = 30, alpha = 0.2,linestyle = \"--\", color = \"blue\")\nplt.axvline(x = 48.5, alpha = 0.3,linestyle = \"--\", color = \"red\")","metadata":{"id":"tETJloaYqrs_","outputId":"380c472a-059e-489f-fb78-a912e7f97994","execution":{"iopub.status.busy":"2022-03-04T14:22:19.985438Z","iopub.execute_input":"2022-03-04T14:22:19.986187Z","iopub.status.idle":"2022-03-04T14:22:20.952562Z","shell.execute_reply.started":"2022-03-04T14:22:19.986145Z","shell.execute_reply":"2022-03-04T14:22:20.951848Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"This is a nice plot telling us that while EstimatedSalary is a less discriminative variable (we can hardly distinguish if someone purchased based solely on it. The only region that we can do so is from above 106000 - the horizontal line -, where everyone purchased!), the Age has 2 separate regions: below 30y.o. and above 49 y.o. \n\nTo have a better understanding of that, check the graph below.\n","metadata":{"id":"HL2PD65Q3QyZ"}},{"cell_type":"code","source":"\ng = sns.jointplot(data=data, x=\"Age\", y=\"EstimatedSalary\", hue = \"Purchased\")\n\ng.plot_joint(sns.kdeplot, color=\"r\", zorder=0, levels=6)\ng.plot_marginals(sns.rugplot, color=\"r\", height=-0.15, clip_on=False)\n\nplt.gcf().set_size_inches(19, 10)","metadata":{"id":"rVcLRLJyvlmJ","outputId":"eb8724be-5df4-444e-bae8-3dbc780dc2da","execution":{"iopub.status.busy":"2022-03-04T14:22:20.953569Z","iopub.execute_input":"2022-03-04T14:22:20.954252Z","iopub.status.idle":"2022-03-04T14:22:22.089438Z","shell.execute_reply.started":"2022-03-04T14:22:20.954206Z","shell.execute_reply":"2022-03-04T14:22:22.088448Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"Just for fun, let's try to visualize it in a 3D plot!","metadata":{"id":"vj6NYz5UEViR"}},{"cell_type":"code","source":"cols = np.sort(data.EstimatedSalary.unique())\n\ndt = pd.DataFrame(index = [data.Age.unique()], columns = cols)\ndt.sort_index(inplace=True)\n\nval = len(data.loc[(data.Age == 50) & (data.EstimatedSalary == 28000)])\n\n\nidx = dt.index.values\n\nfor col in cols:\n  for age in idx:\n    dt.loc[age,col] = len(data.loc[(data.Age == age[0]) & (data.EstimatedSalary == col)])\n\n# dt","metadata":{"id":"LqXIiye39fTV","execution":{"iopub.status.busy":"2022-03-04T14:22:22.090950Z","iopub.execute_input":"2022-03-04T14:22:22.091442Z","iopub.status.idle":"2022-03-04T14:22:25.705639Z","shell.execute_reply.started":"2022-03-04T14:22:22.091399Z","shell.execute_reply":"2022-03-04T14:22:25.704718Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,10))\nfig = go.Figure(data=[go.Surface(z=dt.values)])\n\nfig.update_traces(contours_z=dict(show=True, usecolormap=True,\n                                  highlightcolor=\"limegreen\", project_z=True))\n\nfig.update_layout(title='3D plot distribution of Age and Estimated Salary', autosize=False,\n                  width=1000, height=1000,\n                  margin=dict(l=65, r=50, b=100, t=90))\n\n\nfig.show()","metadata":{"id":"Z6DqQikwchm-","outputId":"4a787f94-5b62-4027-ea55-1bfce9213b52","execution":{"iopub.status.busy":"2022-03-04T14:22:25.707012Z","iopub.execute_input":"2022-03-04T14:22:25.707334Z","iopub.status.idle":"2022-03-04T14:22:26.067241Z","shell.execute_reply.started":"2022-03-04T14:22:25.707294Z","shell.execute_reply":"2022-03-04T14:22:26.066248Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"REMARK: you may have noticed that the axes values are not correspondent to the real values! This is because it's taking as input the length of the variables and not their discrete values! In other words, the values were somehow scaled! ","metadata":{"id":"s7tS_SqrEimS"}},{"cell_type":"markdown","source":"What about the gender and Age? ","metadata":{"id":"sa9Llj3x8XRs"}},{"cell_type":"code","source":"plt.figure(figsize=(15,10))\nplt.title(\"Distribution plot by age and gender\")\nsns.kdeplot(\n   data=data, x=\"Age\", hue=\"Gender\",\n   fill=True, common_norm=False, palette=\"dark\",\n   alpha=.6, linewidth=0,\n)\nplt.axvline(data.Age.mean(), alpha = 0.4, color = \"black\", linestyle = '--', label = \"Age mean\")","metadata":{"id":"7XMX9cZwrIP-","outputId":"b4c30aa0-cd9f-4341-e722-07a73f84b27f","execution":{"iopub.status.busy":"2022-03-04T14:22:26.068627Z","iopub.execute_input":"2022-03-04T14:22:26.068895Z","iopub.status.idle":"2022-03-04T14:22:26.411693Z","shell.execute_reply.started":"2022-03-04T14:22:26.068864Z","shell.execute_reply":"2022-03-04T14:22:26.410627Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"Age and Gender have approximately the same distribution. That means a dataset well balanced for these variables. The dashed black vertical line is the mean of Age (considering both genders)","metadata":{"id":"053z1yqN72So"}},{"cell_type":"markdown","source":"Lets now proceed to train a model and see how it can perform with a simple Logist Regression approach. The idea is to classify if someone will purchase or not based on one or more variables of interest.","metadata":{"id":"p7UhqQgu8hIx"}},{"cell_type":"code","source":"# Divide the data to training set and test set\n# Multivariate model:\nX_train, X_test, y_train, y_test = train_test_split(data.loc[:,['Age',\"EstimatedSalary\"]], data['Purchased'], test_size=0.20)\n# Univariate model: \n# X_train, X_test, y_train, y_test = train_test_split(data.loc[:,[\"EstimatedSalary\"]], data['Purchased'], test_size=0.20)","metadata":{"id":"aR2cDflMWUCo","execution":{"iopub.status.busy":"2022-03-04T14:22:26.413232Z","iopub.execute_input":"2022-03-04T14:22:26.413684Z","iopub.status.idle":"2022-03-04T14:22:26.422343Z","shell.execute_reply.started":"2022-03-04T14:22:26.413628Z","shell.execute_reply":"2022-03-04T14:22:26.421138Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"The variables that I will take into account here are Age and EstimatedSalary. So this is the data that our model will take in order to predict if someone will purchase.","metadata":{"id":"wkdT5azz85cc"}},{"cell_type":"markdown","source":"I will train with 320 samples and test the model on 80!","metadata":{"id":"l0RCzFPh9hza"}},{"cell_type":"code","source":"X_train.shape, X_test.shape\n# y_train.shape","metadata":{"id":"XTQ6vQMGkAqW","outputId":"4534c7b2-b4ae-4560-9136-0a89b10ee800","execution":{"iopub.status.busy":"2022-03-04T14:22:26.426239Z","iopub.execute_input":"2022-03-04T14:22:26.426554Z","iopub.status.idle":"2022-03-04T14:22:26.436340Z","shell.execute_reply.started":"2022-03-04T14:22:26.426520Z","shell.execute_reply":"2022-03-04T14:22:26.435448Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"import math\n\ndef prediction(X,theta,mode):\n    if mode == \"batch\":\n        return np.array([1/(1 + exp(-(np.dot(x,theta))))for x in X])\n\n    elif mode == \"stoc\":\n        return 1/(1 + exp(-(theta[0] + theta[1]*x)))\n\n\ndef cost(x, y, theta,mode,pred):\n    m = x.shape[0]\n    \n    if mode == \"batch\":\n        return (1/m) * sum(np.transpose(y).dot(np.log(pred)) + (1 - np.transpose(y).dot(np.log(1 - pred))))\n#     elif mode == \"stoc\":\n        \ndef grad(x,y,theta,mode):\n    epochs = 600   \n    lr = 0.03\n    m = x.shape[0]\n    y = np.reshape(y,(y.shape[0],1))\n\n    if x.shape[1] < 1:\n      xb = np.reshape(x, (x.shape[0],1))\n      xb = np.append(np.ones((xb.shape[0],1)),xb,axis=1)\n\n    else:\n      xb = np.append(np.ones((x.shape[0],1)),x,axis=1)\n\n    J = []\n    theta = np.array(np.zeros(xb.shape[1])).reshape((xb.shape[1],1))\n\n    for _ in range(epochs):\n        if mode == \"batch\":\n            # display(type(x))\n            pred = prediction(xb,theta,\"batch\")\n            pred = np.reshape(pred,(pred.shape[0],1))\n\n            error = 1/(2*m)*np.transpose((pred - y)).dot(xb)\n            theta = theta - lr*np.reshape(error, (xb.shape[1],1))\n\n            J.append(cost(xb,y,theta,\"batch\",pred)[0])\n\n        elif mode == \"stoc\":\n#             print(\"Our initial theta : \", theta)\n            for i in range(m):\n                pred = prediction(x[i],theta,\"stoc\")\n                J.append((1/(2) * (y[i] - pred)**2))\n                temp0 =1/m *(pred - y[i])\n                temp1 = 1/m * (pred - y[i])*x[i]\n\n                theta = theta - np.array([[temp0[0]],[temp1[0]]])*lr\n\n    plt.figure(figsize=(15,5))\n    plt.plot(J)\n    plt.title(\"Cost over time ( epochs )\")\n    plt.ylabel(\"Cost\")\n    plt.xlabel(\"epochs\")\n    return theta,xb,y\n\nx = np.array(X_train)\ny = np.array(y_train)\nx.shape\nx = x/x.mean()\nx\nst = time.time()\nt,x,y = grad(x,y,0,\"batch\")\nend = time.time()\nprint(\"Time consumed : \", np.round((end-st), 3))\n# grad(x,y,0,\"stoc\")","metadata":{"id":"UrUXLOv0trEv","outputId":"50168d35-e226-4720-f20e-aa7c754719e3","execution":{"iopub.status.busy":"2022-03-04T14:22:26.439316Z","iopub.execute_input":"2022-03-04T14:22:26.439642Z","iopub.status.idle":"2022-03-04T14:22:27.116400Z","shell.execute_reply.started":"2022-03-04T14:22:26.439610Z","shell.execute_reply":"2022-03-04T14:22:27.115547Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"Now let's define a good threshold to establish which will be the value upon our model characterizes a sample as 1 or 0 (purchased or not purchased). First of all, check the balance of such values in our data: 😲","metadata":{"id":"hzuCFMMWIbPa"}},{"cell_type":"code","source":"data.Purchased.value_counts()","metadata":{"id":"uiBjcAQkIV41","outputId":"2487ea7d-8c6d-4deb-807e-8640e1099304","execution":{"iopub.status.busy":"2022-03-04T14:22:27.117767Z","iopub.execute_input":"2022-03-04T14:22:27.120405Z","iopub.status.idle":"2022-03-04T14:22:27.127779Z","shell.execute_reply.started":"2022-03-04T14:22:27.120366Z","shell.execute_reply":"2022-03-04T14:22:27.126921Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"We can see that we have an umbalanced dataset, with near twice samples labeled 0. That said, we can't simple use a value of 0.5 for our threshold. Let's take a look at our predictions statistics to better know where the majority of them falls in: 🤽","metadata":{"id":"UcgFRnfiIrSf"}},{"cell_type":"code","source":"import seaborn as sns\nimport statistics\n\npred = prediction(x,t,\"batch\")\n\nplt.figure(figsize=(15,5))\nsns.distplot(pred, kde=True)\nplt.axvline(x=pred.mean(),\n            color='red', label = \"mean\")\n\nplt.axvline(x=statistics.median(pred),\n            color='orange', label = \"median\")\n\nplt.legend()\nplt.title(\"Predictions distributed plot\")\nplt.figure(figsize=(15,5))\nsns.boxplot(pred)\nsns.swarmplot(pred,color=\".25\")\n\n\n# pred = np.array([1 if x >= 0.5 else 0 for x in pred])\nprint(\"Predictions mean : \", np.round(pred.mean(),3),\"\\nPredictions min value : \", np.round(pred.min(),3),\"\\nPredictions maximum value : \", np.round(pred.max(),3),\"\\nPredictions standard deviation : \"\n      , np.round(pred.std(),3),\"\\nPredictions median : \",np.round(statistics.median(pred),3),\"\\nPredictions 3rd quartile: \",np.round(np.quantile(pred,0.75),3))\n# pred = np.array([1 if x >= pred.mean() else 0 for x in pred])\n","metadata":{"id":"jd7LFhjJl099","outputId":"a9b9d334-9ec1-471d-a65a-4ef3a46d3951","execution":{"iopub.status.busy":"2022-03-04T14:22:27.129130Z","iopub.execute_input":"2022-03-04T14:22:27.129915Z","iopub.status.idle":"2022-03-04T14:22:27.649364Z","shell.execute_reply.started":"2022-03-04T14:22:27.129871Z","shell.execute_reply":"2022-03-04T14:22:27.648446Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"Ok! So that means 75% of our predictions are below the value of 0.445! If we have a ratio between positive and negative values of 143/257 = 0.56 (or 257/400 = 0.64 -> where 400 is the total samples in the dataset) we should take a threshold value near of the 3rd quartile so to cover the \"most part of our positive cases\" (in a statistical term) in the whole dataset. \n\n\nWould this interpretation be correct? You to think about it! ⛹","metadata":{"id":"nYxWt7UMJjh-"}},{"cell_type":"code","source":"pred = np.array([1 if x >= np.quantile(pred,0.75) else 0 for x in pred])\n\ny = np.reshape(y,(y.shape[0],))\na = np.sum(y == pred) / len(y)\nprint(\"Our final accuracy for training  : \", np.round(a,3))\n\nxt = np.array(X_test)\nyt = np.array(y_test)\n\nxt = xt/xt.mean()\nxt = np.append(np.ones((xt.shape[0],1)),xt,axis=1)\n\npredtest = prediction(xt, t, \"batch\")\npredtest = np.array([1 if x >= np.quantile(predtest,0.75) else 0 for x in predtest])\nyt = np.reshape(yt,(yt.shape[0],))\nat = np.sum(yt == predtest)/len(yt)\nprint(\"Our final accuracy for test : \", np.round(at,3))","metadata":{"id":"N9Oc4wJBJS0u","outputId":"7fb43b68-7285-4b45-e7fa-4df914b4bf15","execution":{"iopub.status.busy":"2022-03-04T14:22:27.650634Z","iopub.execute_input":"2022-03-04T14:22:27.650941Z","iopub.status.idle":"2022-03-04T14:22:27.698048Z","shell.execute_reply.started":"2022-03-04T14:22:27.650910Z","shell.execute_reply":"2022-03-04T14:22:27.697127Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# BootStrap Sampling","metadata":{"id":"3KsdwP_n52BV"}},{"cell_type":"markdown","source":"What if we want to do a bootstrap sampling? Learn and test several times models in smaller portions of the dataset? This is quite convinient for big dataset (not the case here) and is always an interest approach.\n\nIn this case, I'm gonna use a Decision Tree Classifier. Is a fast dirty model that can easily learn. It can be used both for Classification and Regression problems!\n\nAdvantages: \n\n1.   Low computational cost\n2.   Little data preparation\n\nDisavantages:\n\n1.   Can easily overfit, thus, not generalizing well\n2.   Find the optmum decision tree can lead to a non global solution (falling in a local solution) -> can be mitigated by using a bag of trees (e.g. Random Forest Classifier)\n3.   Necessary to balance the data, otherwhise, it will be biased!\n\n\n\n","metadata":{"id":"-uMmsHRq54zJ"}},{"cell_type":"markdown","source":"I will thus perform a bootstrap sampling with only 10% of the dataset!! \nOur model will learn the data with only 40 samples!! The idea is to grab randomly 40 samples of the whole dataset with replacement (these samples can be chosen in the next time)\n\nWe repeat this process 10 times and reach an accuracy of 84% and near 95% of time ⏰ reduction!!","metadata":{"id":"S4jDAu_4-OKk"}},{"cell_type":"code","source":"#### Bootstrap sampling\n\nfrom sklearn.utils import resample\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nfrom matplotlib import pyplot\nimport time\nimport random\n\n# random.seed(1234)\n\niter = 10 #### design sampling\n\nstats = []\n\nx = data.loc[:,[\"Age\",\"EstimatedSalary\",\"Purchased\"]]\nn_size = int(0.1*len(x))\n\nx = x.values\n\nst = time.time()\nfor _ in range(iter):\n\n  train = resample(x,n_samples=n_size)\n\n  test = np.array([row for row in x if row.tolist() not in train.tolist()])#.reshape((200,1))\n\n  model = DecisionTreeClassifier(random_state = 1234)\n  model.fit(train[:,:-1],train[:,-1])\n\n  pred = model.predict(test[:,:-1])\n  score = accuracy_score(test[:,-1],pred)\n  # score\n  stats.append(score)\n\nend = time.time()\nprint(\"Time consumed : \", np.round((end - st),3))","metadata":{"id":"2gYCOu9VZEiy","outputId":"dadb6f34-fc87-427b-f1b1-e3933a0a59a2","execution":{"iopub.status.busy":"2022-03-04T14:22:27.699054Z","iopub.execute_input":"2022-03-04T14:22:27.699272Z","iopub.status.idle":"2022-03-04T14:22:27.898643Z","shell.execute_reply.started":"2022-03-04T14:22:27.699246Z","shell.execute_reply":"2022-03-04T14:22:27.897579Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"⏰","metadata":{"id":"aOgb8MZYI47h"}},{"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.displot(stats,kde=True).set(title = \"Distribution of Accuracy for different models\").set(xlabel=\"Accuracy\")\nprint(\"Mean of accuracies : \",np.round(np.mean(stats),3),\"\\nStandard Deviation of accuracies : \", np.round(np.std(stats),3))","metadata":{"id":"W1Vv7Sz_ptRz","outputId":"9c239922-4dc1-45c6-804a-c4ace61e4883","execution":{"iopub.status.busy":"2022-03-04T14:22:27.899743Z","iopub.execute_input":"2022-03-04T14:22:27.899957Z","iopub.status.idle":"2022-03-04T14:22:28.230132Z","shell.execute_reply.started":"2022-03-04T14:22:27.899932Z","shell.execute_reply":"2022-03-04T14:22:28.229231Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"# Confidence Interval\n\nIt may be important, when performing statistical analyses, to consider a maring of error because we performances can always vary as the data been used can be different and so the model's parameters (like its weights). That said, we can't say or model will always have 84% of accuracy. It can vary as you saw in the graph above.\n\nTo do so, as the parameter of interest here is the mean of the accuracies over this repeated process, we shall use the t-Student distribution.\n\n\nThat said, our estimated accuracry for our entire population will be:\n\n  EstAcc = MeanAcc +- t * std / sqrt(n)\n\n  or\n\n  EstAcc = MeanAcc +- MoE\n\nwhere:\n\n*   MeanAcc is the mean of the accuracies calculated previously\n*   t is the value obtained from the table below\n*   std is the Standard Deviation calculated previously\n*   n is the size of our sample (10% of the dataset = 40 samples)\n*   MoE is the \"margin of error\"\n\nto get the t value: we set a confidence interval (95%) and the degree of freedom (n - 1 = 39)\n\n\n---\n<img src = \"https://raw.githubusercontent.com/danielfurlan/aisite/master/LogisticRegression/t-table.png\" align = \"center\" width = \"1000\" >\n\n\n---\nt ~ 2.022\n\nMoE = 2.022 * 0.045 / sqrt(40)\n\nMoE = 0.014 (1.4%)\n\nEstAcc = 84% +- 1.4%\n\nWe can say thus: \nWith 95% confidence we can say that the accuracy of the model representing the whole dataset will fall in the interval 82.6% and 85.4%\n\nIn other words, if we repeat this process many times, each of them grabing randomly 40 samples of the whole dataset, we would have for 95% of the cases a mean accuracy falling in between 82.6% and 85.4%\n\n(For the sake of simplicity, we are assuming that we have at least 10 positive cases and 10 negative cases in each bag of the 40 sampes. Furthermore, we assume that our accuracies follow a normal distribution)\n","metadata":{"id":"7zax3N5Q_BoG"}}]}