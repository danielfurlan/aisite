{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Logistic Regression \n\n","metadata":{"id":"gC5z6Kr8Pa1E"}},{"cell_type":"markdown","source":"\n\n<img src=\"https://raw.githubusercontent.com/danielfurlan/aisite/master/LogisticRegression/dist2d.png\"\nalign = \"left\" width = \"50%\"\n style=\"float:right;width:5px;height:5px;\">\n\n<img src=\"https://raw.githubusercontent.com/danielfurlan/aisite/master/LogisticRegression/shap.png\"\nalign = \"right\" width = \"50%\"\n style=\"float:right;width:5px;height:5px;\">\n <img src=\"https://raw.githubusercontent.com/danielfurlan/aisite/master/LogisticRegression/shapAll.png\"\nalign = \"right\" width = \"50%\" height = \"20%\"\n style=\"float:right;width:5px;height:5px;\">\n\n  <img src=\"https://raw.githubusercontent.com/danielfurlan/aisite/master/LogisticRegression/shap12.png\"\nalign = \"right\" width = \"50%\"\n style=\"float:right;width:5px;height:5px;\">\n","metadata":{"id":"TzNcKPWISDPD"}},{"cell_type":"markdown","source":"# Overview\n\nIn this notebook you will check:\n\n\n1.   Data Visualization\n2.   Build a Logistic Regression from scratch\n3.   a built-in Logistic Regression model from Scikit-Learn\n4.   BootStrap Sampling with Confidence Interval\n5.   Machine Learning Explainability: SHAP values\n\n","metadata":{"id":"V0nnayEyP1sq"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom math import exp\nimport seaborn as sns\nimport plotly.graph_objects as go\nimport time\nimport random\n\nrandom.seed(3)\n\n!wget \"https://drive.google.com/uc?id=15WAD9_4CpUK6EWmgWVXU8YMnyYLKQvW8&export=download\" -O data.csv -q\n# !wget \"https://www.kaggle.com/danielfmfurlan/graduate-admission/data=download\" -O data.csv -q\n# Load the data\ndata = pd.read_csv(\"data.csv\")\ndata.head()","metadata":{"id":"OqDIVXEpWFdN","outputId":"b5e70aac-84ae-4d48-f169-0084c3fdc607","execution":{"iopub.status.busy":"2022-03-06T19:46:11.686046Z","iopub.execute_input":"2022-03-06T19:46:11.686589Z","iopub.status.idle":"2022-03-06T19:46:14.403920Z","shell.execute_reply.started":"2022-03-06T19:46:11.686441Z","shell.execute_reply":"2022-03-06T19:46:14.402952Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"data.describe()","metadata":{"id":"djXVP5PG28Hd","outputId":"ae6ea40d-4d88-4135-8665-7c3c7947af99","execution":{"iopub.status.busy":"2022-03-06T19:46:14.406232Z","iopub.execute_input":"2022-03-06T19:46:14.406478Z","iopub.status.idle":"2022-03-06T19:46:14.442121Z","shell.execute_reply.started":"2022-03-06T19:46:14.406450Z","shell.execute_reply":"2022-03-06T19:46:14.441516Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Lets take a look at some graphs to better visualize these statistics","metadata":{"id":"3xvNrd1j3Imo"}},{"cell_type":"code","source":"plt.figure(figsize=(15,10))\nplt.title(\"Distribution plot for 2 dimensions\")\nsns.kdeplot(\n    data=data, x=\"Age\", y=\"EstimatedSalary\", hue=\"Purchased\",\n    levels=75, thresh=0.2, fill = False, cmap = \"mako\"\n)\nsns.set_style(\"dark\")\nplt.axhline(y = 106000, alpha = 0.3,linestyle = \"--\")\nplt.axvline(x = 30, alpha = 0.2,linestyle = \"--\", color = \"blue\")\nplt.axvline(x = 48.5, alpha = 0.3,linestyle = \"--\", color = \"red\")","metadata":{"id":"tETJloaYqrs_","outputId":"fad91499-870a-488c-c9cf-d98b6e6d4188","execution":{"iopub.status.busy":"2022-03-06T19:46:14.443045Z","iopub.execute_input":"2022-03-06T19:46:14.443292Z","iopub.status.idle":"2022-03-06T19:46:15.463028Z","shell.execute_reply.started":"2022-03-06T19:46:14.443264Z","shell.execute_reply":"2022-03-06T19:46:15.462238Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"This is a nice plot telling us that while EstimatedSalary is a less discriminative variable (we can hardly distinguish if someone purchased based solely on it. The only region that we can do so is from above 106000 - the horizontal line -, where everyone purchased!), the Age has 2 separate regions: below 30y.o. and above 49 y.o. \n\nTo have a better understanding of that, check the graph below.\n","metadata":{"id":"HL2PD65Q3QyZ"}},{"cell_type":"code","source":"\ng = sns.jointplot(data=data, x=\"Age\", y=\"EstimatedSalary\", hue = \"Purchased\")\n\ng.plot_joint(sns.kdeplot, color=\"r\", zorder=0, levels=6)\ng.plot_marginals(sns.rugplot, color=\"r\", height=-0.15, clip_on=False)\n\nplt.gcf().set_size_inches(19, 10)","metadata":{"id":"rVcLRLJyvlmJ","outputId":"e3921edd-5c56-460c-d8c7-be1c5dda36b1","execution":{"iopub.status.busy":"2022-03-06T19:46:15.464420Z","iopub.execute_input":"2022-03-06T19:46:15.464820Z","iopub.status.idle":"2022-03-06T19:46:16.776888Z","shell.execute_reply.started":"2022-03-06T19:46:15.464786Z","shell.execute_reply":"2022-03-06T19:46:16.775652Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"Just for fun, let's try to visualize it in a 3D plot!","metadata":{"id":"vj6NYz5UEViR"}},{"cell_type":"code","source":"cols = np.sort(data.EstimatedSalary.unique())\n\ndt = pd.DataFrame(index = [data.Age.unique()], columns = cols)\ndt.sort_index(inplace=True)\n\nval = len(data.loc[(data.Age == 50) & (data.EstimatedSalary == 28000)])\n\n\nidx = dt.index.values\n\nfor col in cols:\n  for age in idx:\n    dt.loc[age,col] = len(data.loc[(data.Age == age[0]) & (data.EstimatedSalary == col)])\n\n# dt","metadata":{"id":"LqXIiye39fTV","execution":{"iopub.status.busy":"2022-03-06T19:46:16.779830Z","iopub.execute_input":"2022-03-06T19:46:16.780178Z","iopub.status.idle":"2022-03-06T19:46:20.558704Z","shell.execute_reply.started":"2022-03-06T19:46:16.780143Z","shell.execute_reply":"2022-03-06T19:46:20.557579Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,10))\nfig = go.Figure(data=[go.Surface(z=dt.values)])\n\nfig.update_traces(contours_z=dict(show=True, usecolormap=True,\n                                  highlightcolor=\"limegreen\", project_z=True))\n\nfig.update_layout(title='3D plot distribution of Age and Estimated Salary', autosize=False,\n                  width=1000, height=1000,\n                  margin=dict(l=65, r=50, b=100, t=90))\n\n\nfig.show()","metadata":{"id":"Z6DqQikwchm-","outputId":"7f11ef91-28b5-497c-f506-ad37bf29321a","execution":{"iopub.status.busy":"2022-03-06T19:46:20.559791Z","iopub.execute_input":"2022-03-06T19:46:20.560016Z","iopub.status.idle":"2022-03-06T19:46:20.924384Z","shell.execute_reply.started":"2022-03-06T19:46:20.559989Z","shell.execute_reply":"2022-03-06T19:46:20.923294Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"<img src=\"https://raw.githubusercontent.com/danielfurlan/aisite/master/LogisticRegression/newplot (7).png\"\nalign = \"left\" width = \"50%\">\n <img src=\"https://raw.githubusercontent.com/danielfurlan/aisite/master/LogisticRegression/newplot (11).png\"\nalign = \"right\" width = \"50%\">","metadata":{"id":"YbUWbdvPMiej"}},{"cell_type":"markdown","source":"REMARK: you may have noticed that the axes values are not correspondent to the real values! This is because it's taking as input the length of the variables and not their discrete values! In other words, the values were somehow scaled! ","metadata":{"id":"s7tS_SqrEimS"}},{"cell_type":"markdown","source":"What about the gender and Age? ","metadata":{"id":"sa9Llj3x8XRs"}},{"cell_type":"code","source":"plt.figure(figsize=(15,10))\nplt.title(\"Distribution plot by age and gender\")\nsns.kdeplot(\n   data=data, x=\"Age\", hue=\"Gender\",\n   fill=True, common_norm=False, palette=\"dark\",\n   alpha=.6, linewidth=0,\n)\nplt.axvline(data.Age.mean(), alpha = 0.4, color = \"black\", linestyle = '--', label = \"Age mean\")","metadata":{"id":"7XMX9cZwrIP-","outputId":"98cc5628-347e-4881-a1b6-b9a8b39c8907","execution":{"iopub.status.busy":"2022-03-06T19:46:20.925971Z","iopub.execute_input":"2022-03-06T19:46:20.926375Z","iopub.status.idle":"2022-03-06T19:46:21.347089Z","shell.execute_reply.started":"2022-03-06T19:46:20.926269Z","shell.execute_reply":"2022-03-06T19:46:21.345833Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"Age and Gender have approximately the same distribution. That means a dataset well balanced for these variables. The dashed black vertical line is the mean of Age (considering both genders)","metadata":{"id":"053z1yqN72So"}},{"cell_type":"markdown","source":"Lets now proceed to train a model and see how it can perform with a simple Logist Regression approach. The idea is to classify if someone will purchase or not based on one or more variables of interest.","metadata":{"id":"p7UhqQgu8hIx"}},{"cell_type":"code","source":"# Divide the data to training set and test set\n# Multivariate model:\nX_train, X_test, y_train, y_test = train_test_split(data.loc[:,['Age',\"EstimatedSalary\"]], data['Purchased'], test_size=0.20)\n# Univariate model: \n# X_train, X_test, y_train, y_test = train_test_split(data.loc[:,[\"EstimatedSalary\"]], data['Purchased'], test_size=0.20)","metadata":{"id":"aR2cDflMWUCo","execution":{"iopub.status.busy":"2022-03-06T19:46:21.348519Z","iopub.execute_input":"2022-03-06T19:46:21.348758Z","iopub.status.idle":"2022-03-06T19:46:21.360279Z","shell.execute_reply.started":"2022-03-06T19:46:21.348730Z","shell.execute_reply":"2022-03-06T19:46:21.359167Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"The variables that I will take into account here are Age and EstimatedSalary. So this is the data that our model will take in order to predict if someone will purchase.","metadata":{"id":"wkdT5azz85cc"}},{"cell_type":"markdown","source":"I will train with 320 samples and test the model on 80!","metadata":{"id":"l0RCzFPh9hza"}},{"cell_type":"code","source":"X_train.shape, X_test.shape\n# y_train.shape","metadata":{"id":"XTQ6vQMGkAqW","outputId":"2a183d72-2686-40ab-fcec-84d09e925129","execution":{"iopub.status.busy":"2022-03-06T19:46:21.362048Z","iopub.execute_input":"2022-03-06T19:46:21.362393Z","iopub.status.idle":"2022-03-06T19:46:21.374353Z","shell.execute_reply.started":"2022-03-06T19:46:21.362361Z","shell.execute_reply":"2022-03-06T19:46:21.373067Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"import math\n\ndef prediction(X,theta,mode):\n    if mode == \"batch\":\n        return np.array([1/(1 + exp(-(np.dot(x,theta))))for x in X])\n\n    elif mode == \"stoc\":\n        return 1/(1 + exp(-(theta[0] + theta[1]*x)))\n\n\ndef cost(x, y, theta,mode,pred):\n    m = x.shape[0]\n    \n    if mode == \"batch\":\n        return (1/m) * sum(np.transpose(y).dot(np.log(pred)) + (1 - np.transpose(y).dot(np.log(1 - pred))))\n#     elif mode == \"stoc\":\n        \ndef grad(x,y,theta,mode):\n    epochs = 600   \n    lr = 0.03\n    m = x.shape[0]\n    y = np.reshape(y,(y.shape[0],1))\n\n    if x.shape[1] < 1:\n      xb = np.reshape(x, (x.shape[0],1))\n      xb = np.append(np.ones((xb.shape[0],1)),xb,axis=1)\n\n    else:\n      xb = np.append(np.ones((x.shape[0],1)),x,axis=1)\n\n    J = []\n    theta = np.array(np.zeros(xb.shape[1])).reshape((xb.shape[1],1))\n\n    for _ in range(epochs):\n        if mode == \"batch\":\n            # display(type(x))\n            pred = prediction(xb,theta,\"batch\")\n            pred = np.reshape(pred,(pred.shape[0],1))\n\n            error = 1/(2*m)*np.transpose((pred - y)).dot(xb)\n            theta = theta - lr*np.reshape(error, (xb.shape[1],1))\n\n            J.append(cost(xb,y,theta,\"batch\",pred)[0])\n\n        elif mode == \"stoc\":\n#             print(\"Our initial theta : \", theta)\n            for i in range(m):\n                pred = prediction(x[i],theta,\"stoc\")\n                J.append((1/(2) * (y[i] - pred)**2))\n                temp0 =1/m *(pred - y[i])\n                temp1 = 1/m * (pred - y[i])*x[i]\n\n                theta = theta - np.array([[temp0[0]],[temp1[0]]])*lr\n\n    plt.figure(figsize=(15,5))\n    plt.plot(J)\n    plt.title(\"Cost over time ( epochs )\")\n    plt.ylabel(\"Cost\")\n    plt.xlabel(\"epochs\")\n    return theta,xb,y\n\nx = np.array(X_train)\ny = np.array(y_train)\nx.shape\nx = x/x.mean()\nx\nst = time.time()\nt,x,y = grad(x,y,0,\"batch\")\nend = time.time()\nprint(\"Time consumed : \", np.round((end-st), 3))\n# grad(x,y,0,\"stoc\")","metadata":{"id":"UrUXLOv0trEv","outputId":"49ea2cc2-7c49-4d34-e595-0befbc851aa7","execution":{"iopub.status.busy":"2022-03-06T19:46:21.376301Z","iopub.execute_input":"2022-03-06T19:46:21.376631Z","iopub.status.idle":"2022-03-06T19:46:22.422109Z","shell.execute_reply.started":"2022-03-06T19:46:21.376594Z","shell.execute_reply":"2022-03-06T19:46:22.421279Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"Now let's define a good threshold to establish which will be the value upon our model characterizes a sample as 1 or 0 (purchased or not purchased). First of all, check the balance of such values in our data: üò≤","metadata":{"id":"hzuCFMMWIbPa"}},{"cell_type":"code","source":"data.Purchased.value_counts()","metadata":{"id":"uiBjcAQkIV41","outputId":"32ee0598-fc98-47b6-acdb-a3ea77dcc2f0","execution":{"iopub.status.busy":"2022-03-06T19:46:22.423580Z","iopub.execute_input":"2022-03-06T19:46:22.423847Z","iopub.status.idle":"2022-03-06T19:46:22.432492Z","shell.execute_reply.started":"2022-03-06T19:46:22.423817Z","shell.execute_reply":"2022-03-06T19:46:22.431195Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"We can see that we have an umbalanced dataset, with near twice samples labeled 0. That said, we can't simple use a value of 0.5 for our threshold. Let's take a look at our predictions statistics to better know where the majority of them falls in: ü§Ω","metadata":{"id":"UcgFRnfiIrSf"}},{"cell_type":"code","source":"import seaborn as sns\nimport statistics\n\npred = prediction(x,t,\"batch\")\n\nplt.figure(figsize=(15,5))\nsns.distplot(pred, kde=True)\nplt.axvline(x=pred.mean(),\n            color='red', label = \"mean\")\n\nplt.axvline(x=statistics.median(pred),\n            color='orange', label = \"median\")\n\nplt.legend()\nplt.title(\"Predictions distributed plot\")\nplt.figure(figsize=(15,5))\nsns.boxplot(pred)\nsns.swarmplot(pred,color=\".25\")\n\n\n# pred = np.array([1 if x >= 0.5 else 0 for x in pred])\nprint(\"Predictions mean : \", np.round(pred.mean(),3),\"\\nPredictions min value : \", np.round(pred.min(),3),\"\\nPredictions maximum value : \", np.round(pred.max(),3),\"\\nPredictions standard deviation : \"\n      , np.round(pred.std(),3),\"\\nPredictions median : \",np.round(statistics.median(pred),3),\"\\nPredictions 3rd quartile: \",np.round(np.quantile(pred,0.75),3))\n# pred = np.array([1 if x >= pred.mean() else 0 for x in pred])\n","metadata":{"id":"jd7LFhjJl099","outputId":"d40ebfc3-23ad-4c8b-fc5f-e05595b90a9d","execution":{"iopub.status.busy":"2022-03-06T19:46:22.434397Z","iopub.execute_input":"2022-03-06T19:46:22.434773Z","iopub.status.idle":"2022-03-06T19:46:23.205928Z","shell.execute_reply.started":"2022-03-06T19:46:22.434724Z","shell.execute_reply":"2022-03-06T19:46:23.204885Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"Ok! So that means 75% of our predictions are below the value of 0.445! If we have a ratio between positive and negative values of 143/257 = 0.56 (or 257/400 = 0.64 -> where 400 is the total samples in the dataset) we should take a threshold value near of the 3rd quartile so to cover the \"most part of our positive cases\" (in a statistical term) in the whole dataset. \n\n\nWould this interpretation be correct? You to think about it! ‚õπ","metadata":{"id":"nYxWt7UMJjh-"}},{"cell_type":"code","source":"pred = np.array([1 if x >= np.quantile(pred,0.75) else 0 for x in pred])\n\ny = np.reshape(y,(y.shape[0],))\na = np.sum(y == pred) / len(y)\nprint(\"Our final accuracy for training  : \", np.round(a,3))\n\nxt = np.array(X_test)\nyt = np.array(y_test)\n\nxt = xt/xt.mean()\nxt = np.append(np.ones((xt.shape[0],1)),xt,axis=1)\n\npredtest = prediction(xt, t, \"batch\")\npredtest = np.array([1 if x >= np.quantile(predtest,0.75) else 0 for x in predtest])\nyt = np.reshape(yt,(yt.shape[0],))\nat = np.sum(yt == predtest)/len(yt)\nprint(\"Our final accuracy for test : \", np.round(at,3))","metadata":{"id":"N9Oc4wJBJS0u","outputId":"0ece3067-0e8e-43f5-fe96-18f775e962b2","execution":{"iopub.status.busy":"2022-03-06T19:46:23.207568Z","iopub.execute_input":"2022-03-06T19:46:23.207828Z","iopub.status.idle":"2022-03-06T19:46:23.270898Z","shell.execute_reply.started":"2022-03-06T19:46:23.207790Z","shell.execute_reply":"2022-03-06T19:46:23.269704Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# BootStrap Sampling","metadata":{"id":"3KsdwP_n52BV"}},{"cell_type":"markdown","source":"What if we want to do a bootstrap sampling? Learn and test several times models in smaller portions of the dataset? This is quite convinient for big dataset (not the case here) and is always an interest approach.\n\nIn this case, I'm gonna use a Decision Tree Classifier. Is a fast dirty model that can easily learn. It can be used both for Classification and Regression problems!\n\nAdvantages: \n\n1.   Low computational cost\n2.   Little data preparation\n\nDisavantages:\n\n1.   Can easily overfit, thus, not generalizing well\n2.   Find the optmum decision tree can lead to a non global solution (falling in a local solution) -> can be mitigated by using a bag of trees (e.g. Random Forest Classifier)\n3.   Necessary to balance the data, otherwhise, it will be biased!\n\n\n\n","metadata":{"id":"-uMmsHRq54zJ"}},{"cell_type":"markdown","source":"I will thus perform a bootstrap sampling with only 10% of the dataset!! \nOur model will learn the data with only 40 samples!! The idea is to grab randomly 40 samples of the whole dataset with replacement (these samples can be chosen in the next time)\n\nWe repeat this process 10 times and reach an accuracy of 84% and near 95% of time ‚è∞ reduction!!","metadata":{"id":"S4jDAu_4-OKk"}},{"cell_type":"code","source":"#### Bootstrap sampling\n\nfrom sklearn.utils import resample\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nfrom matplotlib import pyplot\nimport time\nimport random\n\n# random.seed(1234)\n\niter = 10 #### design sampling\n\nstats = []\n\nx = data.loc[:,[\"Age\",\"EstimatedSalary\",\"Purchased\"]]\nn_size = int(0.1*len(x))\n\nx = x.values\n\nst = time.time()\nfor _ in range(iter):\n\n  train = resample(x,n_samples=n_size)\n\n  test = np.array([row for row in x if row.tolist() not in train.tolist()])#.reshape((200,1))\n\n  model = DecisionTreeClassifier(random_state = 1234)\n  model.fit(train[:,:-1],train[:,-1])\n\n  pred = model.predict(test[:,:-1])\n  score = accuracy_score(test[:,-1],pred)\n  # score\n  stats.append(score)\n\nend = time.time()\nprint(\"Time consumed : \", np.round((end - st),3))","metadata":{"id":"2gYCOu9VZEiy","outputId":"da1d5d26-3ead-4eaa-e066-24bfca8d755d","execution":{"iopub.status.busy":"2022-03-06T19:46:23.275060Z","iopub.execute_input":"2022-03-06T19:46:23.275366Z","iopub.status.idle":"2022-03-06T19:46:23.515164Z","shell.execute_reply.started":"2022-03-06T19:46:23.275326Z","shell.execute_reply":"2022-03-06T19:46:23.514041Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"‚è∞","metadata":{"id":"aOgb8MZYI47h"}},{"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.displot(stats,kde=True).set(title = \"Distribution of Accuracy for different models\").set(xlabel=\"Accuracy\")\nprint(\"Mean of accuracies : \",np.round(np.mean(stats),3),\"\\nStandard Deviation of accuracies : \", np.round(np.std(stats),3))","metadata":{"id":"W1Vv7Sz_ptRz","outputId":"1d8fd989-3abc-4a44-9297-c9a15b2ed44a","execution":{"iopub.status.busy":"2022-03-06T19:46:23.516792Z","iopub.execute_input":"2022-03-06T19:46:23.517435Z","iopub.status.idle":"2022-03-06T19:46:23.967679Z","shell.execute_reply.started":"2022-03-06T19:46:23.517394Z","shell.execute_reply":"2022-03-06T19:46:23.966485Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{"id":"U3u6P_DeJPMg"}},{"cell_type":"markdown","source":"# Confidence Interval\n\nIt may be important, when performing statistical analyses, to consider a maring of error because we performances can always vary as the data been used can be different and so the model's parameters (like its weights). That said, we can't say or model will always have 84% of accuracy. It can vary as you saw in the graph above.\n\nTo do so, as the parameter of interest here is the mean of the accuracies over this repeated process, we shall use the t-Student distribution.\n\n\nThat said, our estimated accuracry for our entire population will be:\n\n  EstAcc = MeanAcc +- t * std / sqrt(n)\n\n  or\n\n  EstAcc = MeanAcc +- MoE\n\nwhere:\n\n*   MeanAcc is the mean of the accuracies calculated previously\n*   t is the value obtained from the table below\n*   std is the Standard Deviation calculated previously\n*   n is the size of our sample (10% of the dataset = 40 samples)\n*   MoE is the \"margin of error\"\n\nto get the t value: we set a confidence interval (95%) and the degree of freedom (n - 1 = 39)\n\n\n---\n<img src = \"https://raw.githubusercontent.com/danielfurlan/aisite/master/LogisticRegression/t-table.png\" align = \"center\" width = \"1000\" >\n\n\n---\nt ~ 2.022\n\nMoE = 2.022 * 0.045 / sqrt(40)\n\nMoE = 0.014 (1.4%)\n\nEstAcc = 84% +- 1.4%\n\nWe can say thus: \nWith 95% confidence we can say that the accuracy of the model representing the whole dataset will fall in the interval 82.6% and 85.4%\n\nIn other words, if we repeat this process many times, each of them grabing randomly 40 samples of the whole dataset, we would have for 95% of the cases a mean accuracy falling in between 82.6% and 85.4%\n\n(For the sake of simplicity, we are assuming that we have at least 10 positive cases and 10 negative cases in each bag of the 40 sampes. Furthermore, we assume that our accuracies follow a normal distribution)\n","metadata":{"id":"7zax3N5Q_BoG"}},{"cell_type":"code","source":"# !pip install shap","metadata":{"id":"sz6vJP_MY5Nn","execution":{"iopub.status.busy":"2022-03-06T19:46:23.969174Z","iopub.execute_input":"2022-03-06T19:46:23.969559Z","iopub.status.idle":"2022-03-06T19:46:34.657885Z","shell.execute_reply.started":"2022-03-06T19:46:23.969513Z","shell.execute_reply":"2022-03-06T19:46:34.656451Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"# Machine Learning Explainability\n\nGreat about the results. And what if we wanted to understand a bit more of how the model is deciding to which label a sample belongs?\n\nHow to explain a decision made by the model?\n\nSHAP values are an interesting approach to observe how each feature/variable of any sample is contributing to the decision.\nTake a look at it!","metadata":{"id":"B3SxbRARO03e"}},{"cell_type":"code","source":"feature_cols = [\"Age\", \"EstimatedSalary\"]","metadata":{"id":"QtPq8QpnXHpy","execution":{"iopub.status.busy":"2022-03-06T19:46:34.660704Z","iopub.execute_input":"2022-03-06T19:46:34.661030Z","iopub.status.idle":"2022-03-06T19:46:34.666576Z","shell.execute_reply.started":"2022-03-06T19:46:34.660993Z","shell.execute_reply":"2022-03-06T19:46:34.665626Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"import shap\nshap.initjs()\n\ntrain = resample(x,n_samples=n_size, random_state = 1234)\n\ntest = np.array([row for row in x if row.tolist() not in train.tolist()])#.reshape((200,1))\n\nmodel = DecisionTreeClassifier(random_state = 1234)\nmodel.fit(train[:,:-1],train[:,-1])\n\nexplainer = shap.KernelExplainer(model.predict_proba, train[:,:2])\nshap_values = explainer.shap_values(test[:,:2])\nshap.force_plot(explainer.expected_value[0], shap_values[0], test[:,:2], feature_names = feature_cols)","metadata":{"id":"1zHLfQqUYdZW","outputId":"55e0409b-36fd-4cde-c926-f2000c48bd90","execution":{"iopub.status.busy":"2022-03-06T19:46:34.667563Z","iopub.execute_input":"2022-03-06T19:46:34.667786Z","iopub.status.idle":"2022-03-06T19:46:39.092850Z","shell.execute_reply.started":"2022-03-06T19:46:34.667760Z","shell.execute_reply":"2022-03-06T19:46:39.090899Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"How to interpret such values?\n\nFirst of all: \n1.   red means high contribution to label 1 (positive case. Purchased in our case).\n2.   blue means high contribution to label 0 (negative case. Not purchased)\n\nSecondly: the order ot the samples in this case is assembled by SIMILARITY!! You can change it in the horizontal bar.\n\nFinally:\n\nthe width of the colored portions are proportional to how important was that feature to make the model decide. Actually, is all about the values. \nFor instace, the first *127 - 350* samples have both the features (*Age* and *EstimatedSalary*) contributing solely for a high output (remember that our output is a float number between 0 and 1 from our sigmoid function. We convert then this number to integer 0 or 1 according to a threshold). They were all labeled as 1 (Purchased).\n\nOn the other hand, samples ranging from *0 - 76* have **Age** pushing the value to a negative case and **EstimatedSalary** to a positive case! \nBut they haven't enough weight compared to **Age** (netly visible by the difference of width). They were labeled as 0 (not purchased).\n\nFrom *94 - 127* we still have samples labeled as 0 but notice that interestingly, the feature importance switched: *EstimatedSalary* pushing the value to a negative label (0) and *Age* to a positive label (1)!\n\nTo a further interpreation of why this is happening, we should take a look at the spread of such samples with respect to the corelation between *Age* and *EstimatedSalary*.\n\nWhat if we want to see the effect of *Age* through all its possible values? Check below!","metadata":{"id":"FhRshB6GPSIA"}},{"cell_type":"code","source":"shap.initjs()\nshap.force_plot(explainer.expected_value[0], shap_values[0], test[:,:2], feature_names = feature_cols)","metadata":{"id":"kl4RhK5NZtII","outputId":"0be4a1c4-6233-4f7c-eb20-aaf70d21624d","execution":{"iopub.status.busy":"2022-03-06T19:46:39.094827Z","iopub.execute_input":"2022-03-06T19:46:39.095250Z","iopub.status.idle":"2022-03-06T19:46:39.311326Z","shell.execute_reply.started":"2022-03-06T19:46:39.095178Z","shell.execute_reply":"2022-03-06T19:46:39.310320Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"In order to have this image, you just need to switch the horizontal bar to \"Age\". \n\nQuite interesting to notice that while *Age* ranging from *18* to *42* contribute greatly to a *positive* label (1), *Age* above *42* contribute greatly to a *negative* lavel (0). This image is telling us that people with *Age* more than *45* tend to not buy. \n\nI would not take that as granted. Simply because we are dealing with a modelizatino taking into account 2 variables! That said, our model learned the *behaviour* of our data looking at the relation between *Age* and *EstimatedSalary*. \n\nNevertheless, is an indicator.","metadata":{"id":"36x_KYZGa-Td"}},{"cell_type":"markdown","source":"Lets try to see how the *Age* is related to *EstimatedSalary*. We can switch the vertical bar to \"Age effect\" and the horizontal one to \"EstimatedSalary\". That said, we're now looking at how much *effect* *Age* has on the output value through different values of *EstimatedSalary*:","metadata":{"id":"PAmyVpNtcyU3"}},{"cell_type":"code","source":"shap.initjs()\nshap.force_plot(explainer.expected_value[0], shap_values[0], test[:,:2], feature_names = feature_cols)","metadata":{"id":"9UDsb5zAcqXb","outputId":"c2d4a6f2-e349-4ea8-b615-34721f72181d","execution":{"iopub.status.busy":"2022-03-06T19:46:39.315343Z","iopub.execute_input":"2022-03-06T19:46:39.315728Z","iopub.status.idle":"2022-03-06T19:46:39.543233Z","shell.execute_reply.started":"2022-03-06T19:46:39.315680Z","shell.execute_reply":"2022-03-06T19:46:39.541856Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"Would you expect that people earning more would be prone to buy more? Not that simple:\nLet's check all the red portions, where the *Age* contributed to a positive label (1)\n\nThe very first red portion tell us that:\n\n*   *EstimatedSalary* ranging from *15000* to *19000* (the lowest in the data) had a mean *Age* ranging from *26* to *32* y.o. \n\nThe second/third red portion:\n*   *EstimatedSalary* ranging from  *51000* and *72000* ana a mean *Age* ranging from *25* to *38* yo.\n\nThe fourth red portion:\n*   *EstimatedSalary* ranging from  *78000* and *80000* ana a mean *Age* ranging from *31* to *37* yo.\n\nThe fifth red portion:\n*   *EstimatedSalary* ranging from *84000* to *86000* and a mean *Age* ranging from *25* to *27* y.o.\n\n---\n\nLet's now check some blue portions, where the *Age* contributed to a negative label (0)\n\nThe very first blue portion tell us that:\n*   *EstimatedSalary* ranging from *19000* to *27000* had a mean *Age* ranging from *32* to *42* y.o. \n\nThe second blue portion:\n*   *EstimatedSalary* ranging from  *27000* and *31000* and a mean *Age* ranging from *30* to *48* yo.\n\nThe third blue portion:\n*   *EstimatedSalary* ranging from  *31000* and *43000* and a mean *Age* ranging from *35* to *51* yo.\n\nThe greatest effect of *Age* pushing the model to label negative (not purchased), was observed at the pairs (*Age-EstimatedSalary*) **52**-**114000** and **60**-**102000**.\n","metadata":{"id":"9ZJrLBrVdFAh"}},{"cell_type":"markdown","source":"Take a look at the inverse relation now. How much effect has *EstimatedSalary* on the output through all the ages ","metadata":{"id":"8m1TF3UhhBUt"}},{"cell_type":"code","source":"shap.initjs()\nshap.force_plot(explainer.expected_value[0], shap_values[0], test[:,:2], feature_names = feature_cols)","metadata":{"id":"NDEYg01KgvvT","outputId":"cc6ec3ee-144e-45fd-e69c-23c01b01820e","execution":{"iopub.status.busy":"2022-03-06T19:46:39.545282Z","iopub.execute_input":"2022-03-06T19:46:39.546020Z","iopub.status.idle":"2022-03-06T19:46:39.760140Z","shell.execute_reply.started":"2022-03-06T19:46:39.545962Z","shell.execute_reply":"2022-03-06T19:46:39.759304Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"The greatest effect of *EstimatedSalary* pushing the model to label negative (not purchased), was observed at pair (*Age-EstimatedSalary*) **43**-**124000**","metadata":{"id":"ttX7Crg1rsL0"}},{"cell_type":"markdown","source":"And if we took a look at the effect of *EstimatedSalary* on the final output throug all the *EstimatedSalary* values?","metadata":{"id":"9WtFZvCsuSGn"}},{"cell_type":"code","source":"shap.initjs()\nshap.force_plot(explainer.expected_value[0], shap_values[0], test[:,:2], feature_names = feature_cols)","metadata":{"id":"JFTMgG23uaav","outputId":"bba088f5-85eb-4d0a-82eb-1e74e374efc7","execution":{"iopub.status.busy":"2022-03-06T19:46:39.761748Z","iopub.execute_input":"2022-03-06T19:46:39.761975Z","iopub.status.idle":"2022-03-06T19:46:39.975946Z","shell.execute_reply.started":"2022-03-06T19:46:39.761948Z","shell.execute_reply":"2022-03-06T19:46:39.974896Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"Uhm... interesting to observe that:\n\n\n1.   Salary ranging from *15000* to *122000* contributed to a positive label (1) - purchased with only two exceptions from **65000** to **70000** and from **76000** to **80000**\n2.   Salary from *122000* and above contributed to a **negative** label (0)\n\nWe can conclude than that people earning **MORE** don't necessarily purchase more in the used dataset.\n\n","metadata":{"id":"wyLPnfh9umAc"}},{"cell_type":"markdown","source":"What about a single prediction? Take a look at the next image.","metadata":{"id":"QbRSJT19a1Iz"}},{"cell_type":"code","source":"shap.initjs()\ndata_for_prediction =test[20,:2]\ndata_ser = data_for_prediction.reshape(1, -1)\nshap_values = explainer.shap_values(data_ser)\nshap.force_plot(explainer.expected_value[1], shap_values[1], data_ser, feature_names = feature_cols)","metadata":{"id":"BUAK72ZOXDn_","outputId":"41051e32-32c2-4059-8933-b5a923469739","execution":{"iopub.status.busy":"2022-03-06T19:46:39.977755Z","iopub.execute_input":"2022-03-06T19:46:39.978146Z","iopub.status.idle":"2022-03-06T19:46:40.053316Z","shell.execute_reply.started":"2022-03-06T19:46:39.978102Z","shell.execute_reply":"2022-03-06T19:46:40.052280Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"For the sample number 20th in our test dataset, we have this distribution of importance.\n\nAn *Age* of **49** and an *EstimatedSalary* of **28000** were labeled as 1 (purchased). And you can see how important was the *Age* in this decision: it had almost *15* (~75/5) times the importance of *EstimatedSalary*","metadata":{"id":"rpPQieMKYPiB"}},{"cell_type":"markdown","source":"# Conclusion\n\nWe can conclude from the explanations above that while *EstimatedSalary* plays a role in the **particular** model used (*Random Forest Classifier* **with** a **single sampling** of sample size of **40**) *Age* showed to play a major role when predicting if \"someone\" would purchase or not.\n\n\nStill, we observed some particular stratifications either considering *Age* or *EstimatedSalary* (e.g. *15000* to *19000* had a mean *Age* of *26* to *32* and a *positive* label)\n\n\nKeep in mind that if you run this model again with different *random_state* (meaning a different seed to the random generator) you'll have DIFFERENT results!\n\n\nMoreover, if you use a different model, you'll have DIFFERENT results!\n\nThat said, this is a tiny explanation of what this *dataset* **would** represent.\n\n","metadata":{"id":"AGrAnaVos4kY"}}]}